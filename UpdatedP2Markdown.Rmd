---
title: "Project 2 Markdown"
author: "Tommy Papesh, Aidan Hatzer, Jonah Cuenca, Jasper Drumm, Leif Gullstad"
date: "2022-12-8"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Explanation of Data and Business Case
The Data set we chose to use is called “NFL.CSV,” which details the players and player characteristics of everyone who entered the 2009 through 2019 NFL Drafts. Our business question that we hope to answer with our model is: Should a Junior in college declare early for the NFL draft or stay for their senior year? We believe that it is essential for college football players to enter the NFL draft at the optimal time because if they leave early and do not get drafted, they lose out on the ability to ever play college football again or re-enter another NFL draft. Our model hopes to consider specific predictor variables, such as age, height, weight, position, and combine performance to predict if a player is likely to get drafted (our response variable) and help them make a more informed decision on their future and not lose out on college eligibility.

## Normalization Function
```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```

## Data Reading and Cleaning
```{r}
nfl <- read.csv("NFL.csv", stringsAsFactors = TRUE)
# get rid of player name since that is basically like the id column we usually get rid of
# also get rid of the Drafted Information since that will impact our model and is a post-condition of the response variable
nfl$Drafted..tm.rnd.yr. <- NULL
nfl$Player <- NULL
nfl$School <- NULL # removing because there is too many low frequency unique values to run logistic regression

# Set Response variable to 0 and 1
nfl$Drafted <- ifelse(nfl$Drafted == "Yes", 1, 0)

# Changing NA variables to the mean
nfl$Age <- ifelse(is.na(nfl$Age), mean(nfl$Age, na.rm = T), nfl$Age)
nfl$Sprint_40yd <- ifelse(is.na(nfl$Sprint_40yd), mean(nfl$Sprint_40yd, na.rm = T), nfl$Sprint_40yd)
nfl$Bench_Press_Reps <- ifelse(is.na(nfl$Bench_Press_Reps), mean(nfl$Bench_Press_Reps, na.rm = T), nfl$Bench_Press_Reps)
nfl$Vertical_Jump <- ifelse(is.na(nfl$Vertical_Jump), mean(nfl$Vertical_Jump, na.rm = T), nfl$Vertical_Jump)
nfl$Broad_Jump <- ifelse(is.na(nfl$Broad_Jump), mean(nfl$Broad_Jump, na.rm = T), nfl$Broad_Jump)
nfl$Agility_3cone <- ifelse(is.na(nfl$Agility_3cone), mean(nfl$Agility_3cone, na.rm = T), nfl$Agility_3cone)
nfl$Shuttle <- ifelse(is.na(nfl$Shuttle), mean(nfl$Shuttle, na.rm = T), nfl$Shuttle)
summary(nfl)

# Load necessary libraries
library(class)
library(caret)
library(gmodels)
library(neuralnet)
library(kernlab)
library(C50)

# Get the data ready for analysis

# Start by normalizing 
set.seed(12345)
nflmm <- as.data.frame(model.matrix(~.-1,nfl))
nfl_norm <- nflmm
nfl_norm$Age <- normalize(nfl$Age)
nfl_norm$Height <- normalize(nfl$Height)
nfl_norm$Weight <- normalize(nfl$Weight)
nfl_norm$Sprint_40yd <- normalize(nfl$Sprint_40yd)
nfl_norm$Vertical_Jump <- normalize(nfl$Vertical_Jump)
nfl_norm$Bench_Press_Reps <- normalize(nfl$Bench_Press_Reps)
nfl_norm$Broad_Jump <- normalize(nfl$Broad_Jump)
nfl_norm$Agility_3cone <- normalize(nfl$Agility_3cone)
nfl_norm$Shuttle <- normalize(nfl$Shuttle)
nfl_norm$BMI <- normalize(nfl$BMI)
nfl_norm$Year <- normalize(nfl$Year)


sample <- sample(1:nrow(nfl_norm), floor(0.3 * nrow(nfl_norm))) # 30% size of cluster_norm to use for sample
nfl_test <- nfl_norm[sample,] # 30% for test
nfl_train <- nfl_norm[-sample,]
```
## ANN
```{r}
  # ANN_model <- neuralnet(formula = Drafted ~., data = nfl_train, hidden = 1)
  # ANN_pred <- predict(ANN_model, newdata = nfl_test, type = "response")
  # ANN_binary_pred <- ifelse(ANN_pred < 0.5, 0, 1)
  # summary(ANN_binary_pred)
  # summary(ANN_pred)
  # CrossTable(x = nfl_test$Drafted, y = ANN_binary_pred, prop.chisq = F, dnn = c('actual yes', 'predicted yes')) # may have to put binary pred here
  # confusionMatrix(as.factor(nfl_test$Drafted), as.factor(ANN_binary_pred)) # may have to put binary pred here
```
## ANN with Training
```{r, cache=T}
## Customizing the tuning process ----
# use trainControl() to alter resampling strategy
ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")

# customize train() with the control list and grid of parameters 
set.seed(12345)
ann_model <- train(as.factor(Drafted) ~ ., data = nfl_train, method = "nnet",
           metric = "Kappa",
           trControl = ctrl)
ann_model
ann_pred <- predict(ann_model, nfl_test)
CrossTable(x = nfl_test$Drafted, y = ann_pred, prop.chisq = F, dnn = c('actual yes', 'predicted yes'))
confusionMatrix(as.factor(nfl_test$Drafted), as.factor(ann_pred))
```

## ANN Analysis

## Logistic Regression
```{r}
  # LM_model <- glm(Drafted ~ ., data = nfl_train, family = "binomial")
  # LM_pred <- predict(LM_model, newdata = nfl_test, type = "response")
  # LM_binary_pred <- ifelse(LM_pred < 0.5, 0, 1)
  # summary(LM_binary_pred)
  # CrossTable(x = nfl_test$Drafted, y = LM_binary_pred, prop.chisq = F, dnn = c('actual yes', 'predicted yes'))
  # confusionMatrix(as.factor(nfl_test$Drafted), as.factor(LM_binary_pred)) 
```

## Logistic Regression with Training

```{r, cache=T}
## Customizing the tuning process ----
# use trainControl() to alter resampling strategy
ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE", search = "random")

# customize train() with the control list and grid of parameters 
set.seed(12345)
lm <- train(as.factor(Drafted) ~ ., data = nfl_train, method = "glmnet",
           metric = "Kappa",
           trControl = ctrl,
           family = "binomial")
lm
lm_pred <- predict(lm, nfl_test)
table(lm_pred, nfl_test$Drafted)
CrossTable(x = nfl_test$Drafted, y = lm_pred, prop.chisq = F, dnn = c('actual yes', 'predicted yes'))
confusionMatrix(as.factor(nfl_test$Drafted), as.factor(lm_pred))
```


## KNN
```{r, cache=T}
# #labels must be the same length so get them from KNN_norm
# KNN_test_labels <- nfl_norm[sample, "Drafted"]
# KNN_train_labels <- nfl_norm[-sample, "Drafted"]
# 
# 
# #this KNN as a whole tends to have yyes = 0, so I set the k-val arbitrarily to a low number to be able to predict 1's
# k_val <- sqrt(nrow(nfl_train))
# 
# # don't want to make yyes null in tele norm since we need it for SVM and decision trees
# KNN_train <- nfl_train
# KNN_test <- nfl_test
# 
# KNN_train$Drafted <- NULL
# KNN_test$Drafted <- NULL
# 
# KNN_pred <- knn(train = KNN_train, test = KNN_test, cl = KNN_train_labels, k = k_val)
# summary(KNN_pred)
# 
# CrossTable(x = KNN_test_labels, y = KNN_pred, prop.chisq = FALSE)
# confusionMatrix(as.factor(KNN_test_labels), as.factor(KNN_pred))
```

## KNN with Training

```{r}
library(caret)
## Creating a simple tuned model ----
set.seed(12345)
training_control <- trainControl(method = "cv",
                                 number = 10,
                                  selectionFunction = "oneSE")
grid <- expand.grid(k = c(1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50))
knn <- train(as.factor(Drafted) ~ ., data = nfl_train, method = "knn", metric="Accuracy", trControl = training_control, tuneGrid = grid)
knn
knn_pred <- predict(knn, nfl_test)
CrossTable(x = nfl_test$Drafted, y = knn_pred, prop.chisq = F, dnn = c('actual yes', 'predicted yes'))
confusionMatrix(as.factor(nfl_test$Drafted), as.factor(knn_pred))
```

## SVM

```{r}
# #Training a model on the data
# SVM_classifier <- ksvm(as.factor(Drafted) ~ ., data = nfl_train,
#                         kernel = "rbfdot")
# 
# # look at basic information about the model
# SVM_classifier
# # Evaluating model performance 
# # predictions on testing dataset
# SVM_pred <- predict(SVM_classifier, nfl_test)
# summary(SVM_pred)
# 
# table(SVM_pred, nfl_test$Drafted)
# 
# # This just tells us more about the success of the model we built, I think
# # look only at agreement vs. non-agreement
# # construct a vector of TRUE/FALSE indicating correct/incorrect predictions
# agreement <- SVM_pred == nfl_test$Drafted
# table(agreement)
# prop.table(table(agreement))
# 
# CrossTable(x = nfl_test$Drafted, y = SVM_pred, prop.chisq = F, dnn = c('actual yes', 'predicted yes'))
# confusionMatrix(as.factor(nfl_test$Drafted), as.factor(SVM_pred))
```

## SVM with Training

```{r, cache=T}
ctrl <- trainControl(method = "cv", number = 10)
set.seed(12345)

# customize train() with the control list and grid of parameters 
svm_model <- train(as.factor(Drafted) ~ ., data = nfl_train, method = "svmRadial",
           metric = "Kappa",
           trControl = ctrl)
svm_model
svm_pred <- predict(svm_model, nfl_test)
CrossTable(x = nfl_test$Drafted, y = svm_pred, prop.chisq = F, dnn = c('actual yes', 'predicted yes'))
confusionMatrix(as.factor(nfl_test$Drafted), as.factor(svm_pred))
```

## Decision Trees

```{r}
# #Training a model on the data ----
# # build the simplest decision tree
# DT_model <- C5.0(as.factor(Drafted) ~ ., data = nfl_train)
# 
# # display simple facts about the tree
# DT_model
# 
# # display detailed information about the tree
# summary(DT_model)
# 
# plot(DT_model)
# 
# DT_pred <- predict(DT_model, nfl_test)
# summary(DT_pred)
# 
# # cross tabulation of predicted versus actual classes
# CrossTable(x = nfl_test$Drafted, y = DT_pred,
#            prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
#            dnn = c('actual yes', 'predicted yes'))
# confusionMatrix(as.factor(nfl_test$Drafted), as.factor(DT_pred))
```

## Decision Tree with Training

```{r, cache=T}
## Customizing the tuning process ----
# use trainControl() to alter resampling strategy
ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")

# use expand.grid() to create grid of tuning parameters
grid <- expand.grid(.model = "tree",
                    .trials = c(1, 5, 10, 15, 20, 25, 30, 35),
                    .winnow = "FALSE")

# look at the result of expand.grid()
grid

# customize train() with the control list and grid of parameters 
set.seed(12345)
dt_model <- train(as.factor(Drafted) ~ ., data = nfl_train, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)
dt_model
dt_pred <- predict(dt_model, nfl_test)
CrossTable(x = nfl_test$Drafted, y = dt_pred, prop.chisq = F, dnn = c('actual yes', 'predicted yes'))
confusionMatrix(as.factor(nfl_test$Drafted), as.factor(dt_pred))
```

## Second Level Decision Tree

```{r}

Combined_prediction <- data.frame(ann_pred, lm_pred, knn_pred, svm_pred, dt_pred, nfl_test$Drafted)

set.seed(12345)

combined_data <- sample(1:nrow(Combined_prediction), floor(.7 *nrow(Combined_prediction)))

combined_nfl_test <- Combined_prediction[-combined_data,]
combined_nfl_train <- Combined_prediction[combined_data,]

str(combined_nfl_test)



Second_level_DT_model <- C5.0(as.factor(nfl_test.Drafted) ~ ., data = combined_nfl_train)

Second_level_DT_pred <- predict(Second_level_DT_model, combined_nfl_test)

CrossTable(x = combined_nfl_test$nfl_test.Drafted, y = Second_level_DT_pred, prop.chisq = F, dnn = c('actual yes', 'predicted yes'))
confusionMatrix(as.factor(combined_nfl_test$nfl_test.Drafted), as.factor(Second_level_DT_pred))
  
plot(Second_level_DT_model)


```

## Second Level DT with error costs

```{r}

error_cost <- matrix(c(0, 2, 1, 0), nrow = 2)

Second_level_DT_model_with_costs <- C5.0(as.factor(nfl_test.Drafted) ~ ., data = combined_nfl_train, costs = error_cost)

Second_level_DT_with_costs_pred <- predict(Second_level_DT_model_with_costs, combined_nfl_test)

CrossTable(x = combined_nfl_test$nfl_test.Drafted, y = Second_level_DT_with_costs_pred, prop.chisq = F, dnn = c('actual yes', 'predicted yes'))
confusionMatrix(as.factor(combined_nfl_test$nfl_test.Drafted), as.factor(Second_level_DT_with_costs_pred))
  
plot(Second_level_DT_model_with_costs)
```

## Second Level Decision Tree Analysis
  We began by creating 5 simple models (Logistic Regression, ANN, KNN, SVM, and Decision Tree), that each predict if a player is likely to get drafted based on combine characteristics and other physical attributes. Then we made our first second level Decision Tree Model, which had an accuracy of 82%, a kappa value of .4895, a sensitivity of .95, and a specificity of .8059. 
  Because we recognized that it is more detrimental to a college athlete's career and education to declare to the draft and not get drafted than not declaring to the draft if they would have gotten drafted, we decided to make another second level Decision Tree including a cost matrix, which weights false positive errors more heavily. Although this new model's accuracy dropped to 78.27% and its kappa dropped to .4556, our specificity rose to .8319, which means the amount of false positives (model predicts drafted, when player actually would not be) decreased. Our final model includes our first level Decision Tree, our SVM model, and our Logistic Regression model.
    Using our model, college football players would be able to accurately predict whether they will get drafted or not 78.27% of the time. Additionally, there is only a 16.81% chance that a college football player will not be drafted, if our model says he will be. This will help college football players because it will mitigate the losses associated with declaring for the draft too early. Some of the following losses would be: a football player could no longer play at the collegiate level after declaring for the draft, a football player may not be able to complete his degree because he lost his scholarship after declaring, he could lose out on getting drafted next year if he took the time to develop for the next draft. We believe that our model would be very useful to college football players, who currently rely on coaches, advisors, and sports analysts to determine if they should enter the draft, which are less accurate predictors than our model.